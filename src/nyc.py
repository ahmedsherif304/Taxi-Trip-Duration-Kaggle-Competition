# -*- coding: utf-8 -*-
"""nyc_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w6JAUBcnsqqt2OA2-HiNkSBD253MYBhQ

# Initilization
"""

# Commented out IPython magic to ensure Python compatibility.
!!pip install xgboost lightgbm --quiet
import os
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn import svm
from sklearn.metrics import mean_squared_log_error
import pickle
from sklearn.utils import resample
from sklearn.cluster import MiniBatchKMeans
from xgboost import XGBRegressor

from google.colab import drive
drive.mount('/content/drive/')

data_dir = '/content/drive/MyDrive/Machine/data'
os.listdir(data_dir)

train_dir = data_dir + '/train.csv'
train_df = pd.read_csv(train_dir)
train_df.sample(10)

test_dir = data_dir + '/test.csv'
test_df = pd.read_csv(test_dir)
test_df.sample(10)

# Statistical summary of the data
pd.set_option('display.float_format', lambda x: '%.3f' % x)
train_df.describe()

# Get a description of the columns, number of rows, and number of null values.
train_df.info()

"""# Data Cleaning

From the statistical summay, we can see that most of the features are normal except the trip_duration. It has a minimum of 1 second and a maximum of 3526282 seconds (980 hours = 40.8 days). No way someone can take a trip that long using a taxi. Also, a 1 second will not get you anywhere. So, we need to remove the outliers for the trip_duration.

The pickup_datetime feature seems very interesting because we can extract the hour of the day, the day of the week, and the day of the month from it. For example, the trip time differs most likely to be affected if the trip at the peak hour or if it's at a winter month.

The passenger_count feature is an interesting feature because an increased number of passenger may leads to an increased number of stops which may extend the trip time between the starting and ending points.

The vendor_id is a code indicating the provider associated with the trip record. Different vendors may provide different shortest path routes. Some vendor may provide a corrupted information of the shortest path but I think this is highly unlikely to happen.

The store_and_fwd_flag flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server. Maybe there is a relationship between slow trips and server disconnects so we may consider this feature in the training.

The pickup_longitude, pickup_latitude, dropoff_longitude, and dropoff_latitude are the most important features that give value to the model training. We can use them to calculate different types of distances to get a heuristics about the real trip distance which effectively affects the trip duration.

## Remove outliers in the trip duration
"""

# Remove outliers in the trip duration.
# We've decided to exclude data that lies outside n_std standard deviations from the mean. 
# We can try different n_std to see what would be the effect on the end-results.
n_std = 5
train_mean = np.mean(train_df['trip_duration'])
train_std = np.std(train_df['trip_duration'])
train_df = train_df[(train_mean - n_std * train_std <= train_df.trip_duration) & (train_df.trip_duration <= train_mean + n_std * train_std)]
train_df.describe()

"""## Convert dates to pd format"""

# Convert dates to pd format so we can easily manipulate them
train_df.pickup_datetime = pd.to_datetime(train_df.pickup_datetime)
train_df.loc[:, 'pickup_date'] = train_df.pickup_datetime.dt.date
train_df.dropoff_datetime = pd.to_datetime(train_df.dropoff_datetime)

test_df.pickup_datetime = pd.to_datetime(test_df.pickup_datetime)
test_df.loc[:, 'pickup_date'] = test_df.pickup_datetime.dt.date

train_df.sample(10)

"""## Split the date to Month, Day, Week, and Hour"""

# Split the date to Month, Day, Week, and Hour
def split_date(df):
    df['day'] = df['pickup_datetime'].dt.day
    df['hour'] = df['pickup_datetime'].dt.hour
    df['month'] = df['pickup_datetime'].dt.month
    df['week'] = df['pickup_datetime'].dt.dayofweek
    return df

train_df = split_date(train_df)
test_df = split_date(test_df)

train_df.sample(10)

"""# Data Visualization"""

matplotlib.rcParams['figure.figsize'] = (10,6)

def show_hist(col_name):
    plt.hist(train_df[col_name].values, bins=50)
    plt.xlabel(col_name)
    plt.ylabel('count')
    plt.show()

"""## Histograms"""

# Visualize the plain histogram of the trip duration
show_hist('trip_duration')

# Visualize the histogram of the log transformation of the trip duration
# The log transformation may be useful becuase the error metric is the mean squared log error.
train_df['log_trip_duration'] = np.log(train_df['trip_duration'].values + 1)
show_hist('log_trip_duration')

sns.distplot(train_df["log_trip_duration"], bins =50)

"""## Timeseries

It's interesting to visualize the number of trips over time. This could reveal apparent seasonality in the data,  certain trends, and point out any siginficant outliers, and indicate missing values. Also, we can compare the test and train data to see if both of them have the same pattern to ensure uniformity in the test data sample.
"""

plt.plot(train_df.groupby('pickup_date').count()[['id']], 'o-', label='train')
plt.plot(test_df.groupby('pickup_date').count()[['id']], 'o-', label='test')
plt.title('Number of trips over time.')
plt.legend(loc=0)
plt.ylabel('Trips')
plt.show()

"""Both of the train and test data almost have the same pattern. We can see a signficat drop in the number of trips around the last of the Jan. and begining of Feb. and another drop after 4 months.

## Vendors
"""

def plot_bar(data, title, ylabel):
  plt.subplots(1,1,figsize=(17,10))
  plt.ylim(ymin=800)
  plt.ylim(ymax=1100)
  sns.barplot(data.index, data.values)
  plt.title(title)
  plt.legend(loc=0)
  plt.ylabel(ylabel)

vendor_mean = train_df.groupby('vendor_id')['trip_duration'].mean()
plot_bar(vendor_mean, 'Time per Vendor', 'Time in Seconds')

"""It doesn't look that there is a significant difference between the two vendors. They are almost the same.

## Store and Forward
"""

snw_mean = train_df.groupby('store_and_fwd_flag')['trip_duration'].mean()
plot_bar(snw_mean, 'Time per Store and Forward Flag', 'Time in Seconds')

"""There is some difference between the stored trips and the disconnected ones.

## Passenger Count
"""

pc_mean = train_df.groupby('passenger_count')['trip_duration'].mean()
plot_bar(pc_mean, 'Time per Passenger Count', 'Time in Seconds')

"""No significant difference evident that could be explained by the number of passengers in the vehicle for any given trip.

# Distance Calculation

We can calculate the haversine distance which is the great-circle distance between two points on a sphere given their longitudes and latitudes. Also, we can then calculate the summed distance traveled in Manhattan. Finally, we can calculate the direction (or bearing) of the distance traveled. These calculations are stored as variables in the separate data sets.
"""

R = 6371

# The haversine distance which is the great-circle distance between two points on a sphere given their longitudes and latitudes
def haversine_distance(lat1, lng1, lat2, lng2):
    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))
    lat = lat2 - lat1
    lng = lng2 - lng1
    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2
    h = 2 * R * np.arcsin(np.sqrt(d))
    return h

def manhattan_distance(lat1, lng1, lat2, lng2):
    a = haversine_distance(lat1, lng1, lat1, lng2)
    b = haversine_distance(lat1, lng1, lat2, lng1)
    return a + b

def direction(lat1, lng1, lat2, lng2):
    lng_delta_rad = np.radians(lng2 - lng1)
    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))
    y = np.sin(lng_delta_rad) * np.cos(lat2)
    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)
    return np.degrees(np.arctan2(y, x))

train_df.loc[:, 'haversine_distance'] = haversine_distance(train_df.pickup_latitude.values, train_df.pickup_longitude.values, train_df.dropoff_latitude.values, train_df.dropoff_longitude.values)
train_df.loc[:, 'manhattan_distance'] = manhattan_distance(train_df.pickup_latitude.values, train_df.pickup_longitude.values, train_df.dropoff_latitude.values, train_df.dropoff_longitude.values)
train_df.loc[:, 'direction'] = direction(train_df.pickup_latitude.values, train_df.pickup_longitude.values, train_df.dropoff_latitude.values, train_df.dropoff_longitude.values)

test_df.loc[:, 'haversine_distance'] = haversine_distance(test_df.pickup_latitude.values, test_df.pickup_longitude.values, test_df.dropoff_latitude.values, test_df.dropoff_longitude.values)
test_df.loc[:, 'manhattan_distance'] = manhattan_distance(test_df.pickup_latitude.values, test_df.pickup_longitude.values, test_df.dropoff_latitude.values, test_df.dropoff_longitude.values)
test_df.loc[:, 'direction'] = direction(test_df.pickup_latitude.values, test_df.pickup_longitude.values, test_df.dropoff_latitude.values, test_df.dropoff_longitude.values)

train_df.sample(10)

"""# Distance Clustering

We can create neighborhoods of distances. For example, splitting the pickup points into clusters or locations.
"""

# Create the coordinates stack
coords = np.vstack((train_df[['pickup_latitude', 'pickup_longitude']].values, train_df[['dropoff_latitude', 'dropoff_longitude']].values))
# KMeans clustering
indices = np.random.permutation(len(coords))[:500000]
kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[indices])

# Create the clusters
train_df.loc[:, 'pickup_cluster'] = kmeans.predict(train_df[['pickup_latitude', 'pickup_longitude']])
train_df.loc[:, 'dropoff_cluster'] = kmeans.predict(train_df[['dropoff_latitude', 'dropoff_longitude']])
test_df.loc[:, 'pickup_cluster'] = kmeans.predict(test_df[['pickup_latitude', 'pickup_longitude']])
test_df.loc[:, 'dropoff_cluster'] = kmeans.predict(test_df[['dropoff_latitude', 'dropoff_longitude']])

# Visualize the KMeans
fig, ax = plt.subplots(ncols=1, nrows=1)
ax.scatter(train_df.pickup_longitude.values[:500000], train_df.pickup_latitude.values[:500000], s=10, lw=0, c=train_df.pickup_cluster[:500000].values, cmap='autumn', alpha=0.2)
ax.set_xlim((-74.03, -73.75))
ax.set_ylim((40.63, 40.85))
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
plt.show()
train_df.sample(10)

"""# Feature Extraction"""

numerical_cols = ['haversine_distance', 'manhattan_distance', 'direction', 'pickup_cluster', 'dropoff_cluster']
categorical_cols = ['vendor_id', 'passenger_count', 'store_and_fwd_flag', 'day', 'hour', 'month', 'week' ]

# Normalize the numerical columns to make sure they use the same scale
scaler = MinMaxScaler().fit(train_df[numerical_cols])
train_df[numerical_cols] = scaler.transform(train_df[numerical_cols])
test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])

# Convert categorical data into dummy or indicator variables.
# We have few categorical columns and few classes for each column
# So, we can use the OneHotEncoder
def encode_categorical(df):
    for categorical in categorical_cols:
        encoded_cat = pd.get_dummies(df[categorical], prefix=categorical, prefix_sep='_')
        df = (df.drop([categorical], axis=1)).join(encoded_cat)
    return df
train_df = encode_categorical(train_df)
test_df = encode_categorical(test_df)

train_df.sample(10)

train_drop_cols = ['id', 'pickup_datetime', 'dropoff_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'pickup_date', 'log_trip_duration']
train_df = train_df.drop(train_drop_cols, axis = 1)
train_df.sample(10)

test_drop_cols = ['id', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'pickup_date']
test_df = test_df.drop(test_drop_cols, axis = 1)

test_df.sample(10)

# The test_df has 83 columns while train_df has 86 columns 
missed_cols = list(set(train_df.columns) - set(test_df.columns))
missed_cols

test_df['passenger_count_7'] = 0
test_df['passenger_count_8'] = 0
test_df = test_df[list(train_df.columns.values)[1:]]
test_df.sample(10)

plt.figure(figsize= (20,10))
corelation = train_df.corr()
sns.heatmap(abs(corelation), annot=True, cmap='coolwarm')

train_df.to_csv('/content/drive/MyDrive/Machine/proc_data/train_processed.csv', index=False)
test_df.to_csv('/content/drive/MyDrive/Machine/proc_data/test_processed.csv', index=False)

"""# Training"""

def zero_rule(train, test):
  y = train.trip_duration
  baseline_prediction = sum(y) / float(len(y))
  predicted = [baseline_prediction for i in range(len(test))]
  return predicted

train_df.columns

models = {
    "random_forest": RandomForestRegressor(n_jobs = -1, random_state = 42),
    "linear_regression": LinearRegression(),
    "ridge": Ridge(alpha = 0.01),
    "linear_svm": svm.LinearSVR(C = 0.3, max_iter = 10000, epsilon = 0.4, random_state = 42),
    "rbf_svm": svm.SVC(kernel='rbf'),
    "poly_svm": svm.SVC(kernel='poly'),
    "xgboost": XGBRegressor(random_state = 42, n_jobs = -1, gamma = 0.01, learning_rate = 0.01, n_estimators = 200)
}

models['linear_regression'].get_params()

def run_expirement(size, model_name, do_resample = False):
  data = train_df[0:size]
  if do_resample:
    data = resample(train_df, n_samples=size)

  train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)
  
  train_X = train_data.drop(['trip_duration'], axis=1)
  train_y = train_data["trip_duration"]
  val_X = val_data.drop(['trip_duration'], axis=1)
  val_y = val_data["trip_duration"]

  print(train_X.shape, train_y.shape)

  print("Training...")
  model = models[model_name]
  model.fit(train_X, train_y)
  print("Training Done...")

  pickle.dump(model, open('/content/drive/MyDrive/Machine/models_v2/'+str(model_name)+'_'+str(size)+'.sav', 'wb'))
  print("Models Saved...")

  print("Evaluating...")
  score = model.score(val_X, val_y), model.score(train_X, train_y)
  print("Score: ", score)

  print("Prediction...")
  train_preds = model.predict(train_X)
  train_preds = np.where(train_preds < 0, 0, train_preds)
  train_rmsle = mean_squared_log_error(train_y, train_preds, squared=False)
  print(str(size) + " Training RMSLE: ", train_rmsle)

  val_preds = model.predict(val_X)
  val_preds = np.where(val_preds < 0, 0, val_preds)
  val_rmsle = mean_squared_log_error(val_y, val_preds, squared=False)
  print(str(size) + " Validation RMSLE: ", val_rmsle)

def save_submission(size, model_name):
  submission_df = pd.read_csv(data_dir + '/sample_submission.csv')
  model = pickle.load(open('/content/drive/MyDrive/Machine/models_v2/'+str(model_name)+'_'+str(size)+'.sav', 'rb'))
  test_preds = model.predict(test_df)
  submission_df['trip_duration'] = test_preds
  submission_df.to_csv('/content/drive/MyDrive/Machine/data/submission_'+str(model_name)+'_'+str(size)+'.csv', index=None)
  submission_df.sample(10)

# run_expirement(train_df.shape[0], "xgboost")
# save_submission(train_df.shape[0], "xgboost")

models["xgboost"] = XGBRegressor(random_state = 42, n_jobs = -1, learning_rate = 0.2, max_depth = 10, min_samples_ = 40, n_estimators = 200, tree_method = 'gpu_hist')

# run_expirement(train_df.shape[0], "xgboost")
# save_submission(train_df.shape[0], "xgboost")

# run_expirement(train_df.shape[0], "linear_regression")
# save_submission(train_df.shape[0], "linear_regression")

# run_expirement(train_df.shape[0], "linear_svm")
# save_submission(train_df.shape[0], "linear_svm")

# run_expirement(train_df.shape[0], "random_forest")
# save_submission(train_df.shape[0], "random_forest")